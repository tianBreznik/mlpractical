%% REPLACE sXXXXXXX with your student number
\def\studentNumber{sXXXXXXX}


%% START of YOUR ANSWERS
%% Add answers to the questions below, by replacing the text inside the brackets {} for \youranswer{ "Text to be replaced with your answer." }. 
%
% Do not delete the commands for adding figures and tables. Instead fill in the missing values with your experiment results, and replace the images with your own respective figures.
%
% You can generally delete the placeholder text, such as for example the text "Question Figure 3 - Replace the images ..." 
%
% There are 5 TEXT QUESTIONS. Replace the text inside the brackets of the command \youranswer with your answer to the question.
%
% There are also 3 "questions" to replace some placeholder FIGURES with your own, and 1 "question" asking you to fill in the missing entries in the TABLE provided. 
%
% NOTE! that questions are ordered by the order of appearance of their answers in the text, and not necessarily by the order you should tackle them. You should attempt to fill in the TABLE and FIGURES before discussing the results presented there. 
%
% NOTE! If for some reason you do not manage to produce results for some FIGURES and the TABLE, then you can get partial marks by discussing your expectations of the results in the relevant TEXT QUESTIONS. The TABLE specifically has enough information in it already for you to draw meaningful conclusions.
%
% Please refer to the coursework specification for more details.


%% - - - - - - - - - - - - TEXT QUESTIONS - - - - - - - - - - - - 

%% Question 1:
\newcommand{\questionOne} {
\youranswer{
The VGG_08 is a relatively shallow network with only three layers, compared to the much deeper VGG_38 model containing 38 layers. Training the models for 100 epochs, the shallower VGG_08 networks manages steadily reduce the training loss converging to a low error, whilst also performing well on the validation set (Figure 1). The accuracy of the VGG_08 model seems to plateau between 0.5-0.6 (Figure 1), perhaps implying that the network is too shallow for the CIFAR-100 classification task. The VGG_38 model, however, is unable to anything about the information in the dataset, which would enable it to perform better on classification. The loss curves seem to level out at a high value very early on in the training session, whilst the accuracy never improves beyond 0.0 (Figure 1), so the model indeed does not manage to learn anything at all about the data. This indicates that the deeper architecture is not able to use the information capacity of its parameters, to capture features in the data. In the healthy VGG_08 model, the gradients in the initial layers are given higher importance on average than the middle or last layers, illustrating how important it is for the initial layers to capture the main features in the data (Figure 2). The average gradients for the VGG_38 model stagnate at 0 in the first and middle layers, whilst spiking slightly higher in the last two reduction blocks and the last linear prediction output layer (Figure 3). Such results clearly show the presence of the vanishing gradients problem in the deeper VGG_38 model. By the chain rule in backpropagation, the gradients are multiplied through the network from the output to the first layer. In the VGG_38 network this process quite rapidly reduces the gradients to a very small number close to zero or zero, meaning that the weights will not be updated effectively or will not be updated at all. The consequences of this are the model's inability to learn features in the data and hence perform well in the classification task on the training and test sets (Figure 1).}
}

%% Question 2:
\newcommand{\questionTwo} {
\youranswer{Question 2 - Describe Batch Normalization (BN) by using equations. Consider both training and test time and explain how BN addresses the vanishing gradient problem. Note that you are not required to provide the derivation of gradients w.r.t. weights for BN weights.
    The aim of Batch Normalization is to ensure that the distribution of nonlinear inputs remains stable and does not saturate as the network trains. This change in distribution, which causes the internal nodes of a deep network to saturate is called the Internal Covariate Shift, as already mentioned. Fixing this distribution of the layer inputs as the training progresses is expected to improve the training speed. Through introducing the normalization of inputs to each layer, we reduce the damaging effects of the Internal Covariate Shift, by moving towards achieving this fixed distribution of inputs (reference). We normalize each scalar input feature independently, by making it have zero mean and unit variance, meaning that for a layer with an n-dimensional input $x= (x^{(1)} ... x^{(n)})$ each dimension is normalized as: \[\hat{x}^{(k)} = \frac{x^{(k)} - E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}}\]. Here the expectation and variance can be computed over either the entire training dataset or a smaller batch set in stochastic gradient training, in which case each mini-batch produces the mean and variance estimates of every activation. In order to preserev the layer's representational capacity, we ensure that the transformation introduced by the normalization step can represent the identity transform. Two new learnable parameters, learned along with the original model parameters, are introduced for each activation $\gamma^{(k)}$, $\beta^{(k)}$, that in turn scale and shift the normalized input value:\[\gamma^{(k)} = \gamma^{(k)}\hat{x}^{(k)} + \beta^{(k)}\]. For a given mini-batch of input values to a layer, \[\Beta ={x_1,x_2,...,x_m}\] Then, as noted above, the normalized values are $\hat{x}_{1...m}$ and their corresponding linear transformations $y_{1...m}$. The Batch Normalizing transform then consists of the following outputs, based on the learned parameters $\gamma$ and $\beta$:\ \[\mu_{B} \leftarrow \frac{1}{m} \sum_{i=1}^{m} x_i    - the mini-batch mean\] ,  \[\sigma_{B}^2 \leftarrow \frac{1}{m} \sum_{i=1}^{m} (x_i-\mu_{B})^2\] - the mini-batch variance, \[\hat{x}_i \leftarrow \frac{(x_i-\mu_{B})}{\sqrt{\sigma_{B}^2 + \epsilon}}    - the normalized input value,\] \[y_i \leftarrow \gamma\hat{x}_i + \beta    - the Batch Normalizing transform\]. Where we can neglect $\epsilon$, assuming that every element of each mini-batch is sampled from the same distribution. Using this to normalize a network, we pass the normalized input to every layer instead of the original input. This transform is a differentiable function, which introduces normalized activations into the network, so that layers can continue learning on input distributions that suffer from less Internal Covariate Shift, hence preventing small changes to the parameters from amplifying into larger and suboptimal changes in activations in gradients. That is, it mitigates the vanishing gradients problem. At test time, we are interested in how well the model is performing on new, unseen data, so we do not want to transform it. Rather, come test time, we only normalize the inputs, but with the statistics of the entire dataset and not the mini-batch like so:\[\hat{x} = \frac{x - E[x]}{\sqrt{Var[x]} + \epsilon}\]. Here, the means and variances are now fixed. The linear transform with $\gamma$ and $\beta$ can also be applied, but those parameters also remain fixed. Using training set statistics to normalize 

The average length of an answer to this question would be around 2/3 of a column in a 2-column page}
}

%% Question 3:
\newcommand{\questionThree} {
\youranswer{Question 3 - Describe Residual Connections (RC) by using equations. Consider both training and test time and explain how RC address the vanishing gradient problem.
Let us assume that $\mathcal{H}(x)$ is the underlying mapping or hidden representation between the inputs and outputs to a neural net, that it is trying to fit. Introducting residual connections to this network means that instead of making the network fit this underlying function, we instead make it fit a residual mapping, defined as $\mathcal{F}(x) := \mathcal{H}(x) - x$, thus recasting the original mapping into $\mathcal{H}(x) = \mathcal{F}(x) + x$.As mentioned in the previous section, this residual mapping is easier to optimize. This mapping reformulation of $\mathcal{H}(x)$ is incorporated into the network architecture by adding skip connections. These connections can "skip" one or more linear layers at a time, performing a simple identity mapping and adding the outputs to the outputs of the stacked layers, skipped by this mapping. Now let us assume that $x$ is the input to a the first of a stack of a few layers with some underlying mapping $\mathcal{H}(x)$. As we are only performing a linear operation with recasting this mapping into the residual funtion $\mathcal{H}(x) = mathcal{H}(x) - x$, the ability of the network to approximate it asymptotically does not suffer. Stacking layers together and using a redisual connection on them means we bundle them up together into a block, which can be defined as:\[y = \mathcal{F}(x, \{W_i\}) + x\] where x and y are input and output vectors of the layers in the block and $\mathcal{F}(x, \{W_i\})$ is the residual mapping to be learned. The shortcut mapping $\mathcal{F} + x$ is performed by an element-wise addition, to which the non-linear activation function is then applied, so $\sigma(y)$. These operations introduce no new parameters and preserve the existing computational complexity of the network. It is favourable for residual blocks to consist of more than one layer, as with only one layer the residual mapping becomes similar to a linear layer and no performance improvements have been observed for that \cite{bengio1993problem}. So, let us again consider the VGP. It occurs when the elements of the gradient become exponentially small, so that the update of the parameters with the gradient becomes almost insignificant, which means that the network consequently learns very slowly or not at all, considering these changes might be the same magnitude as regular error pertubations in the signals. With skip connections activations from layer $l$ can directly be fed into layer $l+n$, where $n$ is the "depth" of the residual block. This remapping also allows gradients, in turn, to pass uninteruptedly from layer $l+n$ to $l$ during backpropagation. By allowing information to skip layers, layer $l+n$ receives information from both layer $l+t−1$ and layer $l$, hence we are continuosly feeding the output of the blocks with activations from the previous layers and hence preventing the activations from becoming exponentially small, as they never get the chance to become too small. Similarly in backpropagation, the short connections carry the gradient throughout the network depth. 


The average length of an answer to this question would be around 1/2 of a column in a 2-column page}
}

%% Question 4:
\newcommand{\questionFour} {
\youranswer{Question 4 - Present and discuss the experiment results (all of the results and not just the ones you had to fill in) in Table 1 and Figures 4 and 5 (you may use any of the other Figures if you think they are relevant to your analysis). You will have to determine what data are relevant to the discussion, and what information can be extracted from it. Also, discuss what further experiments you would have ran on any combination of VGG08, VGG38, BN, RC in order to
\begin{itemize}
    \item Improve performance of the model trained (explain why you expect your suggested experiments will help with this).
    \item Learn more about the behaviour of BN and RC (explain what you are trying to learn and how).
\end{itemize}

The average length for an answer to this question is approximately 1 of the columns in a 2-column page}
}

%% Question 5:
\newcommand{\questionFive} {
\youranswer{Question 5 - Briefly draw your conclusions based on the results from the previous sections (what are the take-away messages?) and conclude your report with a recommendation for future work. 

Good recommendations for future work also draw on the broader literature (the papers already referenced are good starting points). Great recommendations for future work are not just incremental (an example of an incremental suggestion would be: "we could also train with different learning rates") but instead also identify meaningful questions or, in other words, questions with answers that might be somewhat more generally applicable. 

For example, \citep{huang2017densely} end with \begin{quote}``Because of their compact internal representations and reduced feature redundancy, DenseNets may be good feature extractors for various computer vision tasks that build on convolutional features, e.g.,  [4,5].''\end{quote} 

while \cite{bengio1993problem} state in their conclusions that \begin{quote}``There remains theoretical questions to be considered,  such as whether the problem with simple gradient descent  discussed in this paper would be observed with  chaotic attractors that are not  hyperbolic.\\\end{quote}

The length of this question description is indicative of the average length of a conclusion section}
}

%% - - - - - - - - - - - - FIGURES - - - - - - - - - - - - 

%% Question Figure 3:
\newcommand{\questionFigureThree} {
\youranswer{Question Figure 3 - Replace this image with a figure depicting the average gradient across layers, for the VGG38 model.
%
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{example-image-duck}
    \caption{Gradient Flow on VGG38}
    \label{fig:grad_flow_38}
\end{figure}
}
}

%% Question Figure 4:
\newcommand{\questionFigureFour} {
\youranswer{Question Figure 4 - Replace this image with a figure depicting the training curves for the model with the best performance across experiments you have available. (Also edit the caption accordingly).
%
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{example-image-duck}
    \caption{Training curves for ? ? ?}
    \label{fig:grad_flow_bestModel}
\end{figure}
}
}

%% Question Figure 5:
\newcommand{\questionFigureFive} {
\youranswer{Question Figure 5 - Replace this image with a figure depicting the average gradient across layers, for the model with the best performance across experiments you have available. (Also edit the caption accordingly).
%
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{example-image-duck}
    \caption{Gradient Flow on ? ? ?}
    \label{fig:grad_flow_bestModel}
\end{figure}
}
}

%% - - - - - - - - - - - - TABLES - - - - - - - - - - - - 

%% Question Table 1:
\newcommand{\questionTableOne} {
\youranswer{
Question Table 1 - Fill in Table 1 with the results from your experiments on 
\begin{enumerate}
    \item \textit{VGG38 BN (LR 1e-3)}, and 
    \item \textit{VGG38 BN + RC (LR 1e-2)}.
\end{enumerate}
%
\begin{table*}[t]
    \centering
    \begin{tabular}{lr|ccccc}
    \toprule
        Model                   & LR   & \# Params & Train loss & Train acc & Val loss & Val acc \\
    \midrule
        VGG08                   & 1e-3 & 60 K      &  1.74      & 51.59     & 1.95     & 46.84 \\
        VGG38                   & 1e-3 & 336 K     &  4.61      & 00.01     & 4.61     & 00.01 \\
        VGG38 BN                & 1e-3 & 338 K     &  1.17      & 65.52     & 1.77     & 52.91 \\
        VGG38 RC                & 1e-3 & 336 K     &  1.33      & 61.52     & 1.84     & 52.32 \\
        VGG38 BN + RC           & 1e-3 & 339 K     &  1.26      & 62.99     & 1.73     & 53.76 \\
        VGG38 BN                & 1e-2 & 339 K     &  1.70      & 52.28     & 1.99     & 46.72 \\
        VGG38 BN + RC           & 1e-2 & 339 K     &  0.45      & 85.48     & 1.88     & 58.83 \\
    \bottomrule
    \end{tabular}
    \caption{Experiment results (number of model parameters, Training and Validation loss and accuracy) for different combinations of VGG08, VGG38, Batch Normalisation (BN), and Residual Connections (RC), LR is learning rate.}
    \label{tab:CIFAR_results}
\end{table*} 
}
}

%% END of YOUR ANSWERS